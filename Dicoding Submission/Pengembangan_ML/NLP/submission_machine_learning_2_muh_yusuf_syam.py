# -*- coding: utf-8 -*-
"""Submission_Machine_Learning_2-Muh_Yusuf_Syam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sYXIdS2vG2dTf-WrCPkJ9MuJwmwLBL4R

Import Libararies and Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

import spacy
!python -q -m spacy download en

!pip -q install contractions
import contractions

!pip -q install emoji
import emoji

import re

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

import os
import seaborn as sns
from matplotlib import pyplot as plt
# %matplotlib inline

!pip -q install wordcloud
from wordcloud import WordCloud

"""# Download, and Unzip Dataset

[Dataset Link](https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification)
"""

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d andrewmvd/cyberbullying-classification

data_path= 'cyberbullying-classification'
!unzip 'cyberbullying-classification.zip' -d {data_path}

"""# Load & Explore the Data"""

df= pd.read_csv(os.path.join(data_path, 'cyberbullying_tweets.csv'))
df

df['cyberbullying_type'].value_counts()

df.drop(df[df['cyberbullying_type']=='other_cyberbullying'].index, inplace=True)

df.head()

df= df.drop_duplicates('tweet_text')

df

"""## Visualizing the Wordcloud"""

text= ' '.join(df['tweet_text'])

plt.figure(figsize=(16,10))
wc= WordCloud(width= 800, height= 500, max_words=150, background_color='white').generate(text)
plt.imshow(wc)
plt.axis('off')

"""#**Preprocessing**

## Checking Null Value
"""

df.isna().sum()

"""## Handling Outlier"""

text_len = []
for text in df['tweet_text']:
    tweet_len = len(text.split())
    text_len.append(tweet_len)

df['text_len']= text_len

df.head()

plt.style.use('fivethirtyeight')

plt.figure(figsize=(12, 6))
sns.boxplot(x=df['text_len'], whis=2)

df= df[df['text_len'] > 3]
df= df[df['text_len'] < 100]

df

"""## One Hot Encoding"""

cyberbullying_type= pd.get_dummies(df['cyberbullying_type'])
df_ohe= pd.concat([df['tweet_text'], cyberbullying_type], axis=1)

df_ohe

"""## Train Val Split"""

X= df_ohe['tweet_text']
y= df_ohe.drop('tweet_text', axis=1)

x_train, x_val, y_train, y_val= train_test_split(X, y, test_size=0.2)

len(x_train), len(x_val)

"""## Contraction to Expansion"""

preprocessed_x_train= x_train.map(lambda x: contractions.fix(x.lower()))
preprocessed_x_val= x_val.map(lambda x: contractions.fix(x.lower()))

preprocessed_x_train.head()

"""## Stripping Emojis"""

preprocessed_x_train= preprocessed_x_train.map(lambda x: emoji.replace_emoji(x, replace=''))
preprocessed_x_val= preprocessed_x_val.map(lambda x: emoji.replace_emoji(x, replace=''))

preprocessed_x_train.head()

"""## Clean Hashtags"""

def clean_hashtags(tweet):
    new_tweet = " ".join(word.strip() for word in re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', tweet))
    new_tweet2 = " ".join(word.strip() for word in re.split('#|_', new_tweet))
    return new_tweet2

preprocessed_x_train= preprocessed_x_train.map(clean_hashtags)
preprocessed_x_val= preprocessed_x_val.map(clean_hashtags)

preprocessed_x_train.head()

"""## Remove Mentions and Links"""

preprocessed_x_train= preprocessed_x_train.map(lambda x: re.sub(r'(?:\@|https?\://)\S+', '', x))
preprocessed_x_val= preprocessed_x_val.map(lambda x: re.sub(r'(?:\@|https?\://)\S+', '', x))

preprocessed_x_train.head()

"""## Remove Extra Spaces"""

preprocessed_x_train= preprocessed_x_train.map(lambda x: re.sub('\s\s+', ' ', x))
preprocessed_x_val= preprocessed_x_val.map(lambda x: re.sub('\s\s+', ' ', x))

preprocessed_x_train.head()

"""## Lemmatization"""

nlp= spacy.load('en_core_web_sm')

def make_to_base(x):
    doc= nlp(str(x))
    x_list= [token.lemma_ for token in doc]
        
    return ' '.join(x_list)

"""Cell di bawah memakan waktu 5 menit pada google colabs GPU"""

preprocessed_x_train= preprocessed_x_train.map(make_to_base)
preprocessed_x_val= preprocessed_x_val.map(make_to_base)

preprocessed_x_train.head()

"""## Tokenizing"""

tokenizer = Tokenizer(oov_token='-')

tokenizer.fit_on_texts(preprocessed_x_train)
tokenizer.fit_on_texts(preprocessed_x_val)

preprocessed_x_train = tokenizer.texts_to_sequences(preprocessed_x_train)
preprocessed_x_val = tokenizer.texts_to_sequences(preprocessed_x_val)

"""## Padding"""

words_len=  50

padded_preprocessed_x_train= pad_sequences(preprocessed_x_train, maxlen=words_len, padding='post', truncating = 'post')
padded_preprocessed_x_val= pad_sequences(preprocessed_x_val, maxlen=words_len, padding='post', truncating = 'post')

padded_preprocessed_x_train

"""# Training"""

EMBEDDING_DIM= 256

model = keras.models.Sequential([
    keras.layers.Embedding(len(tokenizer.word_index)+1, EMBEDDING_DIM, input_length=words_len),
    keras.layers.LSTM(64, return_sequences=True),
    keras.layers.Dropout(0.25),
    keras.layers.LSTM(128, return_sequences=True),
    keras.layers.Dropout(0.25),
    keras.layers.LSTM(256),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(256, activation='relu'),
    keras.layers.Dropout(0.25),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.1),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.1),
    keras.layers.Dense(5, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])

reduce_lr= keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor= 0.75, patience=1, min_lr=1e-5, min_delta=0.01)

history = model.fit(padded_preprocessed_x_train, y_train, epochs=10, 
                    validation_data=(padded_preprocessed_x_val, y_val))

"""# Visualization of Loss and Accuracy During Training and Validation."""

plt.figure(figsize=(20, 6))

for idx, i in enumerate(['loss', 'accuracy']):
    plt.subplot(1, 2, idx+1)
    plt.title(i)
    plt.xlabel('Epoch')
    plt.ylabel(i)
    plt.plot(history.history[i], label='Training set')
    plt.plot(history.history[f'val_{i}'], label='Validation set', linestyle='--')
    plt.legend()

plt.show()

"""# Predict a Cyberbullying"""

labels= df_ohe.columns[1:]
labels

text= ['go to kitchen you fucking girl']

x= tokenizer.texts_to_sequences(text)
x= pad_sequences(x, maxlen=words_len, padding='post', truncating = 'post')

pred= model.predict(x)

labels[np.argmax(pred)]